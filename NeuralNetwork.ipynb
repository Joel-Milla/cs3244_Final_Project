{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc64b6c-24f8-414c-ab52-0a9177a514e9",
   "metadata": {},
   "source": [
    "# Neural Networks Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fb003-c400-4fc3-94ef-49ab2996a730",
   "metadata": {},
   "source": [
    "Since the data has shown certain features don't have very correlation, we can argue that using non-linear approximation functions and dense layers would allow us to explore various feature engineering possibilites. This would help us improve the accuracy of our predictions instead of using linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df650890-f73f-465a-8013-fa2b01784b5c",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "For this notebook, I would be using Keras-based Sequential Neural Network along with sklearn-based accuracy metrics to measure the accuracy of my model on the chosen metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "bc197a8a-18cf-4953-bf5d-ca465c670d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation and Manipulation imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Performace Metric imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade9174-94b2-4dbe-b9a2-bc0e650cd790",
   "metadata": {},
   "source": [
    "### Model using pre-processed data\n",
    "\n",
    "I would conduct basic pre-processing and then construct the neural network. The pre-processing step involves encoding categorical variables to train the model. This would help set a baseline to understand the improvements made using feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "1dba163e-9edd-4dbd-9de0-dc06371e8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data and pre-processing the categorical variables\n",
    "\n",
    "data_raw = pd.read_pickle('raw_features.pkl')\n",
    "data_raw.dropna(inplace=True)\n",
    "X = data_raw.drop('Target_Comment_Volume', axis=1)\n",
    "y = data_raw['Target_Comment_Volume']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "db8492f6-b9a4-49d1-9721-a7629c715139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "0460e576-90c9-4c43-adf9-6da173b467d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 399.0063 - mae: 5.5005\n",
      "Test Loss: [416.95806884765625, 5.467024803161621]\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step\n",
      "Test set performance:\n",
      " MAE : 5.4670\n",
      " MSE : 416.9581\n",
      " RMSE: 20.4196\n",
      " R²  : 0.6101\n"
     ]
    }
   ],
   "source": [
    "# Defining the NN model\n",
    "model = keras.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')   # single output for regression\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set performance:\")\n",
    "print(f\" MAE : {mae:.4f}\")\n",
    "print(f\" MSE : {mse:.4f}\")\n",
    "print(f\" RMSE: {rmse:.4f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfa48-cc8f-48da-b555-3574abf037ce",
   "metadata": {},
   "source": [
    "### Model using feature engineering\n",
    "\n",
    "When we provide the neural network more data and extra features, I expect the model to perform better since the model would have more features to form the Dense hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "c53ac3e2-fefe-47de-8875-8818e0efe4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the feature-engineered data\n",
    "train_data = pd.read_csv(\"train_df.csv\")\n",
    "test_data = pd.read_csv(\"test_df.csv\")\n",
    "X_train = train_data.drop('Target_Comment_Volume', axis=1)\n",
    "y_train = train_data['Target_Comment_Volume']\n",
    "X_test = test_data.drop('Target_Comment_Volume', axis=1)\n",
    "y_test = test_data['Target_Comment_Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "d7a3396a-b2a0-4966-9d73-0af14467f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 362.9358 - mae: 4.5947\n",
      "Test Loss: [378.2878723144531, 4.734480381011963]\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437us/step\n",
      "Test set performance:\n",
      " MAE : 4.7345\n",
      " MSE : 378.2878\n",
      " RMSE: 19.4496\n",
      " R²  : 0.6381\n"
     ]
    }
   ],
   "source": [
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.fit_transform(X_test)\n",
    "\n",
    "# Defining the NN model\n",
    "model = keras.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set performance:\")\n",
    "print(f\" MAE : {mae:.4f}\")\n",
    "print(f\" MSE : {mse:.4f}\")\n",
    "print(f\" RMSE: {rmse:.4f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af81e5-e3b3-4c83-8d0a-0244b60313b9",
   "metadata": {},
   "source": [
    "As expected, our model performed the linear models which shows that the most accurate model would use the non-linearity of the data o the MSE of our model has improved significantly after feature engineering the dataset using the manipulations explained in the Feature Engineering Notebook. Since we have more features to work with, the neural network would be able to fit the dataset better. \n",
    "\n",
    "However, a limitation of the neural network is that it tends to overfit the data unless larger datasets are provided. This would mean that the accuracy of our model can't be improved unless we get more data or features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803f2d6-fc99-4a34-8f37-e9e7b2e77a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
