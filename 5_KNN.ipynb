{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18731f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Deep Learning\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28272db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, y_test, y_pred=None):\n",
    "    '''\n",
    "    We test our model and print various metrics for comparison\n",
    "\n",
    "    Params:\n",
    "    model: to test\n",
    "    X_test: which are features to test\n",
    "    y_test: the real values that match X_test\n",
    "    '''\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_pred = np.rint(y_pred).astype(int)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mabse = median_absolute_error(y_test, y_pred)\n",
    "    print(f\"Root mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean absolute Error: {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Median absolute Error: {mabse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f2f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.read_csv('train_df.csv')\n",
    "test_ = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa3b71",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a distance-based algorithm, which means it relies heavily on the geometry of the feature space. When we have high-dimensional data, especially with correlated or less informative features, KNN can perform poorly due to the curse of dimensionality.\n",
    "\n",
    "To address this, we apply Principal Component Analysis (PCA) before KNN:\n",
    "\n",
    "- Reduces dimensionality: Keeps only the most important features (principal components)\n",
    "- Removes noise and redundancy\n",
    "- Improves performance and speed of KNN by working in a cleaner, lower-dimensional space\n",
    "\n",
    "In short, PCA helps KNN make more accurate and efficient predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1c8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Transform(X_train_, X_test_):    \n",
    "    # 2. Standardize features FIRST\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_)\n",
    "    X_test_scaled = scaler.transform(X_test_)\n",
    "    \n",
    "    # 3. THEN apply PCA to scaled data\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Reduced dimensions from {X_train_.shape[1]} to {X_train_pca.shape[1]} features\")\n",
    "    \n",
    "    return (X_train_pca, X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043642c",
   "metadata": {},
   "source": [
    "**Why Use an Autoencoder After PCA?**\n",
    "\n",
    "After applying PCA to reduce the feature space and remove redundancy, we further applied an autoencoder for non-linear feature extraction.\n",
    "\n",
    "While PCA is a linear method, autoencoders are neural networks that can learn complex, non-linear patterns in the data. This helps us capture deeper structure that PCA might miss.\n",
    "\n",
    "Benefits of using both:\n",
    "\n",
    "- PCA simplifies the data and reduces noise\n",
    "- Autoencoder compresses and reconstructs data with non-linear transformations\n",
    "- Together, they give us a compact and powerful feature representation for KNN, which is sensitive to feature quality\n",
    "\n",
    "By stacking PCA and an autoencoder, we combine the strengths of both methods and feed more informative features into our KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7c883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions from 130 to 93 features\n",
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189us/step\n"
     ]
    }
   ],
   "source": [
    "# 1. Separate features and target\n",
    "X_train = train_.drop(columns=['Target_Comment_Volume'])\n",
    "y_train = train_['Target_Comment_Volume']\n",
    "X_test = test_.drop(columns=['Target_Comment_Volume'])\n",
    "y_test = test_['Target_Comment_Volume']\n",
    "\n",
    "# 2. Split data BEFORE fitting the autoencoder\n",
    "X_train_scaled, X_test_scaled = PCA_Transform(X_train, X_test)\n",
    "\n",
    "# 3. Define autoencoder architecture\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 16  # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 4. Train the autoencoder ONLY on the training set\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# 5. Encode features using the trained encoder\n",
    "X_train_enc = encoder.predict(X_train_scaled)\n",
    "X_test_enc = encoder.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0cebf",
   "metadata": {},
   "source": [
    "We aim to evaluate how different K-Nearest Neighbors (KNN) configurations affect regression performance.\n",
    "Specifically, we will experiment with:\n",
    "\n",
    "- Different values for n_neighbors\n",
    "- Different distance metrics such as 'euclidean', 'manhattan', and 'minkowski'\n",
    "This helps identify which combination gives the best predictive results for this dataset.\n",
    "\n",
    "**Why we Used These Parameters**\n",
    "\n",
    "- n_neighbors determines how many neighbors the model considers when making predictions. Testing small and larger values helps balance between bias and variance.\n",
    "- Distance metrics define how similarity between points is measured. Some metrics work better with high-dimensional data or specific feature distributions.\n",
    "\n",
    "**Configurations we Tested**\n",
    "\n",
    "- n_neighbors: 1-20 and √(n_samples)\n",
    "- metric: 'euclidean', 'manhattan', 'uniform' with p=1.5 and 3\n",
    "\n",
    "We kept all other parameters constant while changing one at a time to isolate its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141465ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance metric\n",
      "k = 1, MSE = 843.9022\n",
      "k = 2, MSE = 698.4037\n",
      "k = 3, MSE = 649.2014\n",
      "k = 4, MSE = 632.4292\n",
      "k = 5, MSE = 636.1538\n",
      "k = 6, MSE = 624.6023\n",
      "k = 7, MSE = 611.8633\n",
      "k = 8, MSE = 608.0482\n",
      "k = 9, MSE = 607.8222\n",
      "k = 10, MSE = 603.7797\n",
      "k = 11, MSE = 603.1007\n",
      "k = 12, MSE = 601.8431\n",
      "k = 13, MSE = 603.7869\n",
      "k = 14, MSE = 606.0562\n",
      "k = 15, MSE = 609.2841\n",
      "k = 16, MSE = 608.4124\n",
      "k = 17, MSE = 610.4487\n",
      "k = 18, MSE = 612.2564\n",
      "k = 19, MSE = 615.4605\n",
      "k = 20, MSE = 616.5939\n",
      "k = 180, MSE = 787.3499\n",
      "\n",
      "Best k: 12\n",
      "Root mean Squared Error: 21.1304\n",
      "Mean absolute Error: 4.7114\n",
      "Mean Squared Error: 446.49\n",
      "Median absolute Error: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean distance metric\")\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='euclidean')\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c184889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance metric\n",
      "k = 1, MSE = 843.9022\n",
      "k = 2, MSE = 698.4037\n",
      "k = 3, MSE = 649.2014\n",
      "k = 4, MSE = 632.4292\n",
      "k = 5, MSE = 636.1538\n",
      "k = 6, MSE = 624.6023\n",
      "k = 7, MSE = 611.8633\n",
      "k = 8, MSE = 608.0482\n",
      "k = 9, MSE = 607.8222\n",
      "k = 10, MSE = 603.7797\n",
      "k = 11, MSE = 603.1007\n",
      "k = 12, MSE = 601.8431\n",
      "k = 13, MSE = 603.7869\n",
      "k = 14, MSE = 606.0562\n",
      "k = 15, MSE = 609.2841\n",
      "k = 16, MSE = 608.4124\n",
      "k = 17, MSE = 610.4487\n",
      "k = 18, MSE = 612.2564\n",
      "k = 19, MSE = 615.4605\n",
      "k = 20, MSE = 616.5939\n",
      "k = 180, MSE = 787.3499\n",
      "\n",
      "Best k: 12\n",
      "Root mean Squared Error: 21.6013\n",
      "Mean absolute Error: 4.7466\n",
      "Mean Squared Error: 466.62\n",
      "Median absolute Error: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Manhattan distance metric\")\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='manhattan')\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee1ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minkowski distance with p=1.5 metric\n",
      "k = 1, MSE = 843.9022\n",
      "k = 2, MSE = 698.4037\n",
      "k = 3, MSE = 649.2014\n",
      "k = 4, MSE = 632.4292\n",
      "k = 5, MSE = 636.1538\n",
      "k = 6, MSE = 624.6023\n",
      "k = 7, MSE = 611.8633\n",
      "k = 8, MSE = 608.0482\n",
      "k = 9, MSE = 607.8222\n",
      "k = 10, MSE = 603.7797\n",
      "k = 11, MSE = 603.1007\n",
      "k = 12, MSE = 601.8431\n",
      "k = 13, MSE = 603.7869\n",
      "k = 14, MSE = 606.0562\n",
      "k = 15, MSE = 609.2841\n",
      "k = 16, MSE = 608.4124\n",
      "k = 17, MSE = 610.4487\n",
      "k = 18, MSE = 612.2564\n",
      "k = 19, MSE = 615.4605\n",
      "k = 20, MSE = 616.5939\n",
      "k = 180, MSE = 787.3499\n",
      "\n",
      "Best k: 12\n",
      "Root mean Squared Error: 21.3025\n",
      "Mean absolute Error: 4.7233\n",
      "Mean Squared Error: 453.79\n",
      "Median absolute Error: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Minkowski distance with p=1.5 metric\")\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='minkowski', p=1.5)\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c3760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minkowski distance with p=3 metric\n",
      "k = 1, MSE = 843.9022\n",
      "k = 2, MSE = 698.4037\n",
      "k = 3, MSE = 649.2014\n",
      "k = 4, MSE = 632.4292\n",
      "k = 5, MSE = 636.1538\n",
      "k = 6, MSE = 624.6023\n",
      "k = 7, MSE = 611.8633\n",
      "k = 8, MSE = 608.0482\n",
      "k = 9, MSE = 607.8222\n",
      "k = 10, MSE = 603.7797\n",
      "k = 11, MSE = 603.1007\n",
      "k = 12, MSE = 601.8431\n",
      "k = 13, MSE = 603.7869\n",
      "k = 14, MSE = 606.0562\n",
      "k = 15, MSE = 609.2841\n",
      "k = 16, MSE = 608.4124\n",
      "k = 17, MSE = 610.4487\n",
      "k = 18, MSE = 612.2564\n",
      "k = 19, MSE = 615.4605\n",
      "k = 20, MSE = 616.5939\n",
      "k = 180, MSE = 787.3499\n",
      "\n",
      "Best k: 12\n",
      "Root mean Squared Error: 21.4765\n",
      "Mean absolute Error: 4.7429\n",
      "Mean Squared Error: 461.24\n",
      "Median absolute Error: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Minkowski distance with p=3 metric\")\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='minkowski', p=3)\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa20bd0",
   "metadata": {},
   "source": [
    "BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab4292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared Error: 21.1304\n",
      "Mean absolute Error: 4.7114\n",
      "Mean Squared Error: 446.49\n",
      "Median absolute Error: 1.0000\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=12, metric='euclidean')\n",
    "knn.fit(X_train_enc, y_train)\n",
    "y_pred = knn.predict(X_test_enc)\n",
    "y_pred = np.rint(y_pred).astype(int)\n",
    "test(knn, X_test_enc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e18b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of worst 10 test points: [3713, 1814, 4918, 2142, 6619, 2576, 1696, 3931, 4973, 6214]\n",
      "      y_true  y_pred  error  abs_error\n",
      "3713     695      10   -685        685\n",
      "1814    1059     599   -460        460\n",
      "4918     697     275   -422        422\n",
      "2142     589     173   -416        416\n",
      "6619     441      37   -404        404\n",
      "2576     455      95   -360        360\n",
      "1696     505     154   -351        351\n",
      "3931     363      38   -325        325\n",
      "4973     308       9   -299        299\n",
      "6214     426     137   -289        289\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(X_test_enc)\n",
    "\n",
    "results['y_true']    = y_test.values\n",
    "results['y_pred']    = y_pred\n",
    "results['error']     = results['y_pred'] - results['y_true']\n",
    "results['abs_error'] = results['error'].abs()\n",
    "\n",
    "worst10 = results.nlargest(10, 'abs_error')\n",
    "\n",
    "print(\"Indices of worst 10 test points:\", worst10.index.tolist())\n",
    "print(worst10[['y_true','y_pred','error','abs_error']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd00106",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The best KNN model uses:\n",
    "\n",
    "- 12 nearest neighbors\n",
    "- Euclidean distance metric\n",
    "- Distance-based weighting\n",
    "\n",
    "This configuration outperformed other combinations by giving more influence to closer neighbors, which helped the model better capture local variations in the data. As a result, it produced the most accurate predictions in our evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
