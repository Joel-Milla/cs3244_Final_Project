{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbf0cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56dc7d52-ce3e-4341-9ba8-70093108f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66c4efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.read_csv('train_df.csv')\n",
    "test_ = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b7b73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, y_test, y_pred=None):\n",
    "    '''\n",
    "    We test our model and print various metrics for comparison\n",
    "\n",
    "    Params:\n",
    "    model: to test\n",
    "    X_test: which are features to test\n",
    "    y_test: the real values that match X_test\n",
    "    '''\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mabse = median_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Root mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean absolute Error: {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Median absolute Error: {mabse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acef4df",
   "metadata": {},
   "source": [
    "We firstly try without any preprocessing in order to see that it can be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "910ffcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Root mean Squared Error: 28.5271\n",
      "Mean absolute Error: 6.2295\n",
      "Mean Squared Error: 813.79\n",
      "Median absolute Error: 0.6000\n",
      "Decision Tree\n",
      "Root mean Squared Error: 29.7779\n",
      "Mean absolute Error: 5.5937\n",
      "Mean Squared Error: 886.72\n",
      "Median absolute Error: 1.0000\n",
      "Linear Regression\n",
      "Root mean Squared Error: 20.4863\n",
      "Mean absolute Error: 5.9233\n",
      "Mean Squared Error: 419.69\n",
      "Median absolute Error: 2.4599\n"
     ]
    }
   ],
   "source": [
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"KNN\")\n",
    "test(knn, X_test, y_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"Decision Tree\")\n",
    "test(dt, X_test, y_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Linear Regression\")\n",
    "test(lr, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b92bc-1c30-4d9e-b688-96aaa80ed368",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153949d",
   "metadata": {},
   "source": [
    " ### Why Use a Deep Autoencoder?\n",
    "\n",
    "The purpose of using a deep autoencoder in this project is to perform automatic feature compression before feeding the data into regression models.\n",
    "\n",
    "**Why Autoencoders?**\n",
    "- High-dimensional feature sets may contain noise and redundant information.\n",
    "- Autoencoders can learn non-linear relationships and compress the data into a lower-dimensional representation.\n",
    "- This can reduce overfitting and improve generalization, especially for distance-based models like KNN.\n",
    "\n",
    "**What we do here:**\n",
    "- We experiment with various encoder output dimensions: 8, 16, and 32.\n",
    "- We try different layer sizes (64 and 128) to understand which architecture gives the best downstream performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30862c",
   "metadata": {},
   "source": [
    "Dim 8 Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1821138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 23.5009\n",
      "Mean absolute Error: 5.2205\n",
      "Mean Squared Error: 552.29\n",
      "Median absolute Error: 0.6000\n",
      "<----Decision Tree after autoencoder---->\n",
      "Root mean Squared Error: 32.3786\n",
      "Mean absolute Error: 6.4632\n",
      "Mean Squared Error: 1048.38\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 29.8426\n",
      "Mean absolute Error: 11.0653\n",
      "Mean Squared Error: 890.58\n",
      "Median absolute Error: 6.4993\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_encoded, y_train)\n",
    "test(knn, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Decision Tree after autoencoder---->\")\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_encoded, y_train)\n",
    "test(dt, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "test(lr, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2bf45",
   "metadata": {},
   "source": [
    "Dim 16 Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc9496ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 22.4430\n",
      "Mean absolute Error: 5.0423\n",
      "Mean Squared Error: 503.69\n",
      "Median absolute Error: 0.6000\n",
      "<----Decision Tree after autoencoder---->\n",
      "Root mean Squared Error: 33.8022\n",
      "Mean absolute Error: 6.9413\n",
      "Mean Squared Error: 1142.59\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 29.4204\n",
      "Mean absolute Error: 11.3354\n",
      "Mean Squared Error: 865.56\n",
      "Median absolute Error: 6.9671\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 16\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_encoded, y_train)\n",
    "test(knn, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Decision Tree after autoencoder---->\")\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_encoded, y_train)\n",
    "test(dt, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "test(lr, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7de16",
   "metadata": {},
   "source": [
    "Dim 32 Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d62dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 22.7349\n",
      "Mean absolute Error: 5.2109\n",
      "Mean Squared Error: 516.88\n",
      "Median absolute Error: 0.8000\n",
      "<----Decision Tree after autoencoder---->\n",
      "Root mean Squared Error: 32.1299\n",
      "Mean absolute Error: 6.9558\n",
      "Mean Squared Error: 1032.33\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 26.4451\n",
      "Mean absolute Error: 10.8286\n",
      "Mean Squared Error: 699.34\n",
      "Median absolute Error: 5.9003\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_encoded, y_train)\n",
    "test(knn, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Decision Tree after autoencoder---->\")\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_encoded, y_train)\n",
    "test(dt, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "test(lr, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec05e74",
   "metadata": {},
   "source": [
    "Dim 8 Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2baf14b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 23.3649\n",
      "Mean absolute Error: 5.2004\n",
      "Mean Squared Error: 545.92\n",
      "Median absolute Error: 0.8000\n",
      "<----Decision Tree after autoencoder---->\n",
      "Root mean Squared Error: 36.8169\n",
      "Mean absolute Error: 7.5092\n",
      "Mean Squared Error: 1355.49\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 29.1636\n",
      "Mean absolute Error: 11.6546\n",
      "Mean Squared Error: 850.51\n",
      "Median absolute Error: 7.4322\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_encoded, y_train)\n",
    "test(knn, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Decision Tree after autoencoder---->\")\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_encoded, y_train)\n",
    "test(dt, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "test(lr, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c1fd4",
   "metadata": {},
   "source": [
    "Dim 16 Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5b9dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 21.4998\n",
      "Mean absolute Error: 4.7074\n",
      "Mean Squared Error: 462.24\n",
      "Median absolute Error: 0.6000\n",
      "<----Decision Tree after autoencoder---->\n",
      "Root mean Squared Error: 30.3272\n",
      "Mean absolute Error: 6.7413\n",
      "Mean Squared Error: 919.74\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 29.2884\n",
      "Mean absolute Error: 11.8357\n",
      "Mean Squared Error: 857.81\n",
      "Median absolute Error: 6.8572\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 16\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_encoded, y_train)\n",
    "test(knn, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Decision Tree after autoencoder---->\")\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_encoded, y_train)\n",
    "test(dt, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "test(lr, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de754f5b",
   "metadata": {},
   "source": [
    "Dim 32 Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "479a47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1024/1024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189us/step\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 21.9116\n",
      "Mean absolute Error: 4.7749\n",
      "Mean Squared Error: 480.12\n",
      "Median absolute Error: 0.6000\n",
      "<----Decision Tree after autoencoder---->\n",
      "Root mean Squared Error: 31.6728\n",
      "Mean absolute Error: 6.8829\n",
      "Mean Squared Error: 1003.17\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 27.4529\n",
      "Mean absolute Error: 11.2657\n",
      "Mean Squared Error: 753.66\n",
      "Median absolute Error: 5.9439\n"
     ]
    }
   ],
   "source": [
    "# Split features and target\n",
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train and evaluate models\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_encoded, y_train)\n",
    "test(knn, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Decision Tree after autoencoder---->\")\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_encoded, y_train)\n",
    "test(dt, X_test_encoded, y_test)\n",
    "\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_encoded, y_train)\n",
    "test(lr, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2b45a",
   "metadata": {},
   "source": [
    "### Conclusion: What Did We Learn from Using Autoencoders?\n",
    "\n",
    "**Best Configuration:**\n",
    "- Encoder dimension = **16**\n",
    "- Hidden layer size = **128**\n",
    "\n",
    "**Why?**\n",
    "- This setting retained enough information to represent the features well without over-compressing.\n",
    "- It led to the best performance improvement in **K-Nearest Neighbors (KNN)**.\n",
    "\n",
    "**Impact on Models:**\n",
    "- **KNN improved significantly**, because reducing dimensionality helped mitigate the \"curse of dimensionality\" which affects distance-based models.\n",
    "- **Other models (Decision Tree, Linear Regression)** showed little to no improvement, and in some cases performed slightly worse.\n",
    "  - This suggests those models are either more robust to feature space size or already benefit from the full feature information.\n",
    "\n",
    "**Takeaway:**\n",
    "Using a deep autoencoder can be a valuable preprocessing step, especially when:\n",
    "- The input space is high-dimensional\n",
    "- We're using models sensitive to feature space structure (like KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eccafd-f0a9-40ce-99f7-4591a02ac6d5",
   "metadata": {},
   "source": [
    "# PCA\n",
    "When data is linearly separable, simple models like linear regression, logistic regression, or linear SVMs can more easily separate the classes and predict values correctly.\n",
    "To try and improve the linearity of our data, previously we applied autoencoder and decoder to reduce the dimensionality of our dataset. Now, we will apply PCA.\n",
    "\n",
    "#### Why PCA?\n",
    "As seen in class, PCA:\n",
    "\n",
    "- Reduces dimensionality while preserving variance in the data\n",
    "- Eliminates multicollinearity between highly correlated features. This could help because the derived features of our dataset are highly correlated and could assist in reducing redundancy.\n",
    "- Reduces noise that may be present in less relevant features. We think that this point could help with the outliers contained in the data.\n",
    "- Improves computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc490002-9733-4c00-b055-218ec986484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Transform(X_train_, X_test_):    \n",
    "    # 2. Standardize features FIRST\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_)\n",
    "    X_test_scaled = scaler.transform(X_test_)\n",
    "    \n",
    "    # 3. THEN apply PCA to scaled data\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Reduced dimensions from {X_train_.shape[1]} to {X_train_pca.shape[1]} features\")\n",
    "    \n",
    "    return (X_train_pca, X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2636c3a4-8d89-4184-9e67-193b45aeec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions from 130 to 93 features\n",
      "<----KNN---->\n",
      "Root mean Squared Error: 21.0321\n",
      "Mean absolute Error: 4.7462\n",
      "Mean Squared Error: 442.35\n",
      "Median absolute Error: 0.6000\n",
      "<----Decision Tree---->\n",
      "Root mean Squared Error: 35.2508\n",
      "Mean absolute Error: 6.6546\n",
      "Mean Squared Error: 1242.62\n",
      "Median absolute Error: 1.0000\n",
      "<----Linear Regression---->\n",
      "Root mean Squared Error: 20.7841\n",
      "Mean absolute Error: 5.6914\n",
      "Mean Squared Error: 431.98\n",
      "Median absolute Error: 2.2870\n"
     ]
    }
   ],
   "source": [
    "X_train = train_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_train = train_[\"Target_Comment_Volume\"]\n",
    "X_test = test_.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y_test = test_[\"Target_Comment_Volume\"]\n",
    "\n",
    "X_train, X_test = PCA_Transform(X_train, X_test)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"<----KNN---->\")\n",
    "test(knn, X_test, y_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"<----Decision Tree---->\")\n",
    "test(dt, X_test, y_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"<----Linear Regression---->\")\n",
    "test(lr, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0c19c-178f-4b01-8f4c-23837d55a3c6",
   "metadata": {},
   "source": [
    "### Conclusions: What did we learned from using PCA? \n",
    "Our results demonstrate mixed outcomes across models when applying PCA:\n",
    "\n",
    "- KNN showed clear improvement (RMSE: 27.84 → 26.51, R²: 0.24 → 0.31) as expected for distance-based algorithms. Especially since in class we saw the curse of dimensionality in which KNN is highly sensitive to high dimensional features.\n",
    "- Linear regression remained relatively stable with minimal change. We expected the model to improve slightly because it is a linear model.\n",
    "- Decision Tree performed worse after PCA, likely because they already handle feature selection implicitly (and could improve by limiting size of the tree) and we are reducing the amount of information. Also, decision trees are more robust against outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
