{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18731f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Deep Learning\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b4f98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('raw_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28272db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, y_test, y_pred=None):\n",
    "    '''\n",
    "    We test our model and print various metrics for comparison\n",
    "\n",
    "    Params:\n",
    "    model: to test\n",
    "    X_test: which are features to test\n",
    "    y_test: the real values that match X_test\n",
    "    '''\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mabse = median_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Root mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean absolute Error: {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Median absolute Error: {mabse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b84a5",
   "metadata": {},
   "source": [
    "In Data Preproccesing step, we found that Auto Encoder and PCA are useful for KNN model. Thus, we decided to use Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a1c8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Transform(X_train_, X_test_):    \n",
    "    # 2. Standardize features FIRST\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_)\n",
    "    X_test_scaled = scaler.transform(X_test_)\n",
    "    \n",
    "    # 3. THEN apply PCA to scaled data\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Reduced dimensions from {X_train_.shape[1]} to {X_train_pca.shape[1]} features\")\n",
    "    \n",
    "    return (X_train_pca, X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c7c883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions from 42 to 18 features\n",
      "\u001b[1m896/896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166us/step\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step\n"
     ]
    }
   ],
   "source": [
    "# 1. Separate features and target\n",
    "X = data.drop(columns=[\"Target_Comment_Volume\"])\n",
    "y = data[\"Target_Comment_Volume\"]\n",
    "\n",
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Split data BEFORE fitting the autoencoder\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_train_scaled, X_test_scaled = PCA_Transform(X_train_scaled, X_test_scaled)\n",
    "\n",
    "\n",
    "# 4. Define autoencoder architecture\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32  # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 5. Train the autoencoder ONLY on the training set\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# 6. Encode features using the trained encoder\n",
    "X_train_enc = encoder.predict(X_train_scaled)\n",
    "X_test_enc = encoder.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0cebf",
   "metadata": {},
   "source": [
    "We aim to evaluate how different K-Nearest Neighbors (KNN) configurations affect regression performance.\n",
    "Specifically, we will experiment with:\n",
    "\n",
    "- Different values for n_neighbors\n",
    "- Different distance metrics such as 'euclidean', 'manhattan', and 'minkowski'\n",
    "This helps identify which combination gives the best predictive results for this dataset.\n",
    "\n",
    "**Why we Used These Parameters**\n",
    "\n",
    "- n_neighbors determines how many neighbors the model considers when making predictions. Testing small and larger values helps balance between bias and variance.\n",
    "- Distance metrics define how similarity between points is measured. Some metrics work better with high-dimensional data or specific feature distributions.\n",
    "\n",
    "**Configurations we Tested**\n",
    "\n",
    "- n_neighbors: 1- 20 and √(n_samples)\n",
    "- metric: 'euclidean', 'manhattan'\n",
    "- weights: 'uniform', 'distance'\n",
    "\n",
    "We kept all other parameters constant while changing one at a time to isolate its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "141465ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, MSE = 1042.1933\n",
      "k = 2, MSE = 786.5828\n",
      "k = 3, MSE = 731.6285\n",
      "k = 4, MSE = 702.4147\n",
      "k = 5, MSE = 691.4351\n",
      "k = 6, MSE = 681.4678\n",
      "k = 7, MSE = 677.9552\n",
      "k = 8, MSE = 669.9713\n",
      "k = 9, MSE = 674.6091\n",
      "k = 10, MSE = 671.9347\n",
      "k = 11, MSE = 674.1292\n",
      "k = 12, MSE = 670.0896\n",
      "k = 13, MSE = 673.1724\n",
      "k = 14, MSE = 672.3092\n",
      "k = 15, MSE = 675.7894\n",
      "k = 16, MSE = 675.0683\n",
      "k = 17, MSE = 676.0422\n",
      "k = 18, MSE = 677.6937\n",
      "k = 19, MSE = 680.3035\n",
      "k = 20, MSE = 680.4171\n",
      "k = 169, MSE = 834.5759\n",
      "\n",
      "Best k: 8\n",
      "Root mean Squared Error: 20.8284\n",
      "Mean absolute Error: 4.6458\n",
      "Mean Squared Error: 433.82\n",
      "Median absolute Error: 0.6250\n",
      "R² Score: 0.5728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='euclidean', weights='uniform')\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d9d65f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, MSE = 1042.1933\n",
      "k = 2, MSE = 786.5828\n",
      "k = 3, MSE = 731.6285\n",
      "k = 4, MSE = 702.4147\n",
      "k = 5, MSE = 691.4351\n",
      "k = 6, MSE = 681.4678\n",
      "k = 7, MSE = 677.9552\n",
      "k = 8, MSE = 669.9713\n",
      "k = 9, MSE = 674.6091\n",
      "k = 10, MSE = 671.9347\n",
      "k = 11, MSE = 674.1292\n",
      "k = 12, MSE = 670.0896\n",
      "k = 13, MSE = 673.1724\n",
      "k = 14, MSE = 672.3092\n",
      "k = 15, MSE = 675.7894\n",
      "k = 16, MSE = 675.0683\n",
      "k = 17, MSE = 676.0422\n",
      "k = 18, MSE = 677.6937\n",
      "k = 19, MSE = 680.3035\n",
      "k = 20, MSE = 680.4171\n",
      "k = 169, MSE = 834.5759\n",
      "\n",
      "Best k: 8\n",
      "Root mean Squared Error: 20.5049\n",
      "Mean absolute Error: 4.5695\n",
      "Mean Squared Error: 420.45\n",
      "Median absolute Error: 0.6299\n",
      "R² Score: 0.5860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='euclidean', weights='distance')\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c184889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, MSE = 1042.1933\n",
      "k = 2, MSE = 786.5828\n",
      "k = 3, MSE = 731.6285\n",
      "k = 4, MSE = 702.4147\n",
      "k = 5, MSE = 691.4351\n",
      "k = 6, MSE = 681.4678\n",
      "k = 7, MSE = 677.9552\n",
      "k = 8, MSE = 669.9713\n",
      "k = 9, MSE = 674.6091\n",
      "k = 10, MSE = 671.9347\n",
      "k = 11, MSE = 674.1292\n",
      "k = 12, MSE = 670.0896\n",
      "k = 13, MSE = 673.1724\n",
      "k = 14, MSE = 672.3092\n",
      "k = 15, MSE = 675.7894\n",
      "k = 16, MSE = 675.0683\n",
      "k = 17, MSE = 676.0422\n",
      "k = 18, MSE = 677.6937\n",
      "k = 19, MSE = 680.3035\n",
      "k = 20, MSE = 680.4171\n",
      "k = 169, MSE = 834.5759\n",
      "\n",
      "Best k: 8\n",
      "Root mean Squared Error: 21.4117\n",
      "Mean absolute Error: 4.6789\n",
      "Mean Squared Error: 458.46\n",
      "Median absolute Error: 0.6250\n",
      "R² Score: 0.5485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='manhattan', weights='uniform')\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "86ca2ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, MSE = 1042.1933\n",
      "k = 2, MSE = 786.5828\n",
      "k = 3, MSE = 731.6285\n",
      "k = 4, MSE = 702.4147\n",
      "k = 5, MSE = 691.4351\n",
      "k = 6, MSE = 681.4678\n",
      "k = 7, MSE = 677.9552\n",
      "k = 8, MSE = 669.9713\n",
      "k = 9, MSE = 674.6091\n",
      "k = 10, MSE = 671.9347\n",
      "k = 11, MSE = 674.1292\n",
      "k = 12, MSE = 670.0896\n",
      "k = 13, MSE = 673.1724\n",
      "k = 14, MSE = 672.3092\n",
      "k = 15, MSE = 675.7894\n",
      "k = 16, MSE = 675.0683\n",
      "k = 17, MSE = 676.0422\n",
      "k = 18, MSE = 677.6937\n",
      "k = 19, MSE = 680.3035\n",
      "k = 20, MSE = 680.4171\n",
      "k = 169, MSE = 834.5759\n",
      "\n",
      "Best k: 8\n",
      "Root mean Squared Error: 20.9398\n",
      "Mean absolute Error: 4.5954\n",
      "Mean Squared Error: 438.47\n",
      "Median absolute Error: 0.6244\n",
      "R² Score: 0.5682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='manhattan', weights='distance')\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eee1ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, MSE = 1042.1933\n",
      "k = 2, MSE = 786.5828\n",
      "k = 3, MSE = 731.6285\n",
      "k = 4, MSE = 702.4147\n",
      "k = 5, MSE = 691.4351\n",
      "k = 6, MSE = 681.4678\n",
      "k = 7, MSE = 677.9552\n",
      "k = 8, MSE = 669.9713\n",
      "k = 9, MSE = 674.6091\n",
      "k = 10, MSE = 671.9347\n",
      "k = 11, MSE = 674.1292\n",
      "k = 12, MSE = 670.0896\n",
      "k = 13, MSE = 673.1724\n",
      "k = 14, MSE = 672.3092\n",
      "k = 15, MSE = 675.7894\n",
      "k = 16, MSE = 675.0683\n",
      "k = 17, MSE = 676.0422\n",
      "k = 18, MSE = 677.6937\n",
      "k = 19, MSE = 680.3035\n",
      "k = 20, MSE = 680.4171\n",
      "k = 169, MSE = 834.5759\n",
      "\n",
      "Best k: 8\n",
      "Root mean Squared Error: 20.6796\n",
      "Mean absolute Error: 4.5719\n",
      "Mean Squared Error: 427.64\n",
      "Median absolute Error: 0.6256\n",
      "R² Score: 0.5789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "try_k = int(np.sqrt(len(X_train_enc)))\n",
    "\n",
    "best_k = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Try different k values\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, metric='euclidean', weights='distance')\n",
    "    scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_score = -scores.mean()\n",
    "    print(f\"k = {k}, MSE = {mean_score:.4f}\")\n",
    "    \n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_k = k\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=try_k, metric='euclidean', weights='distance')\n",
    "scores = cross_val_score(knn, X_train_enc, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "mean_score = -scores.mean()\n",
    "print(f\"k = {try_k}, MSE = {mean_score:.4f}\")\n",
    "\n",
    "if mean_score < best_score:\n",
    "    best_score = mean_score\n",
    "    best_k = k\n",
    "\n",
    "print(f\"\\nBest k: {best_k}\")\n",
    "\n",
    "# Train final model with best k\n",
    "final_knn = KNeighborsRegressor(n_neighbors=best_k, metric='minkowski', weights='distance', p=1.5)\n",
    "final_knn.fit(X_train_enc, y_train)\n",
    "test(final_knn, X_test_enc, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa20bd0",
   "metadata": {},
   "source": [
    "BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bab4292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared Error: 20.5049\n",
      "Mean absolute Error: 4.5695\n",
      "Mean Squared Error: 420.45\n",
      "Median absolute Error: 0.6299\n",
      "R² Score: 0.5860\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=8, metric='euclidean', weights='distance')\n",
    "knn.fit(X_train_enc, y_train)\n",
    "test(knn, X_test_enc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd00106",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The best KNN model uses:\n",
    "\n",
    "- 8 nearest neighbors\n",
    "- Euclidean distance metric\n",
    "- Distance-based weighting\n",
    "\n",
    "This configuration outperformed other combinations by giving more influence to closer neighbors, which helped the model better capture local variations in the data. As a result, it produced the most accurate predictions in our evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
