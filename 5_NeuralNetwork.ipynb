{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc64b6c-24f8-414c-ab52-0a9177a514e9",
   "metadata": {},
   "source": [
    "# Neural Networks Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fb003-c400-4fc3-94ef-49ab2996a730",
   "metadata": {},
   "source": [
    "Since the data shows that certain features have low correlation, we can argue that using non-linear approximation functions and dense layers would allow us to explore various feature engineering possibilities. This would help us improve the accuracy of our predictions, rather than using linear models.\n",
    "\n",
    "The neural network would work with a multi-layer perceptron network consisting of 3 layers: the input layer with 64 nodes, the hidden layer with 32 nodes, and the output layer with one node. The activation function used for the hidden layers would be ReLU, as it helps analyze non-linear feature trends. The output layer, on the other hand, would use a linear activation function for regression results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df650890-f73f-465a-8013-fa2b01784b5c",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "For this notebook, I would be using Keras-based Sequential Neural Network along with sklearn-based accuracy metrics to measure the accuracy of my model on the chosen metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc197a8a-18cf-4953-bf5d-ca465c670d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation and Manipulation imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Performace Metric imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model imports\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade9174-94b2-4dbe-b9a2-bc0e650cd790",
   "metadata": {},
   "source": [
    "### Model using pre-processed data\n",
    "\n",
    "I would conduct basic pre-processing and then construct the neural network. The pre-processing step involves encoding categorical variables to train the model. This would help set a baseline to understand the improvements made using feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dba163e-9edd-4dbd-9de0-dc06371e8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data and pre-processing the categorical variables\n",
    "\n",
    "data_raw = pd.read_csv(\"Preprocessed_Data.csv\")\n",
    "data_raw.dropna(inplace=True)\n",
    "X = data_raw.drop('Target_Comment_Volume', axis=1)\n",
    "y = data_raw['Target_Comment_Volume']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db8492f6-b9a4-49d1-9721-a7629c715139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0460e576-90c9-4c43-adf9-6da173b467d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 430.1505 - mae: 5.0373\n",
      "Test Loss: [434.134033203125, 5.081584930419922]\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step\n",
      "Test set performance:\n",
      " MAE : 5.0816\n",
      " MSE : 434.1341\n",
      " RMSE: 20.8359\n",
      " Median Absolute Error: 1.6213\n"
     ]
    }
   ],
   "source": [
    "# Defining the NN model\n",
    "model = keras.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')   # single output for regression\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "meae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Test set performance:\")\n",
    "print(f\" MAE : {mae:.4f}\")\n",
    "print(f\" MSE : {mse:.4f}\")\n",
    "print(f\" RMSE: {rmse:.4f}\")\n",
    "print(f\" Median Absolute Error: {meae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfa48-cc8f-48da-b555-3574abf037ce",
   "metadata": {},
   "source": [
    "### Model using feature engineering\n",
    "\n",
    "When we provide the neural network more data and extra features, I expect the model to perform better since the model would have more features to form the Dense hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c53ac3e2-fefe-47de-8875-8818e0efe4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the feature-engineered data\n",
    "train_data = pd.read_csv(\"train_df.csv\")\n",
    "test_data = pd.read_csv(\"test_df.csv\")\n",
    "X_train = train_data.drop('Target_Comment_Volume', axis=1)\n",
    "y_train = train_data['Target_Comment_Volume']\n",
    "X_test = test_data.drop('Target_Comment_Volume', axis=1)\n",
    "y_test = test_data['Target_Comment_Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7a3396a-b2a0-4966-9d73-0af14467f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 341.8941 - mae: 4.5799\n",
      "Test Loss: [407.0761413574219, 4.6044230461120605]\n",
      "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step\n",
      "Test set performance:\n",
      " MAE : 4.6044\n",
      " MSE : 407.0761\n",
      " RMSE: 20.1761\n",
      " Median Absolute Error: 0.9682\n"
     ]
    }
   ],
   "source": [
    "# 2. Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.fit_transform(X_test)\n",
    "\n",
    "# Defining the NN model\n",
    "model = keras.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Training the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "meae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Test set performance:\")\n",
    "print(f\" MAE : {mae:.4f}\")\n",
    "print(f\" MSE : {mse:.4f}\")\n",
    "print(f\" RMSE: {rmse:.4f}\")\n",
    "print(f\" Median Absolute Error: {meae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af81e5-e3b3-4c83-8d0a-0244b60313b9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As expected, our model performed the linear models, which shows that the most accurate model would use the non-linearity of the data to improve accuracy. The MSE of our model has improved significantly after feature engineering the dataset using the manipulations explained in the Feature Engineering Notebook. Since we have more features to work with, the neural network would be able to fit the dataset better. \n",
    "\n",
    "However, a limitation of the neural network is that it tends to overfit the data unless larger datasets are provided. This would mean that the accuracy of our model can't be improved unless we get more data or features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d494c-df28-4fb8-8d89-c0d5d8e974cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
