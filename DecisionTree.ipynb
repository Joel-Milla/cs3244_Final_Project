{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe938a63-a3af-4d3a-8f14-6f8e9c933ac6",
   "metadata": {},
   "source": [
    "## Import variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74c1e4a0-a51d-4ae4-a369-fce1ba8cc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147380a-8e00-4169-977d-fdaf369b00af",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d514f71-d3c4-4408-ad5b-1e36fcd0b5e1",
   "metadata": {},
   "source": [
    "Paper: https://uksim.info/uksim2015/data/8713a015.pdf\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/dataset/363/facebook+comment+volume+dataset\n",
    "\n",
    "The following dataset contains instances of facebook posts. The task is to predict how many comments the post will receive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5c0dc-daef-4ad7-bed3-f76f70165458",
   "metadata": {},
   "source": [
    "We first load the whole training dataset with its columns. We join the training and testing set into one, as we will be manually dividing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0fd8c7-4256-4dd8-8e28-c8be16bf3122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50993, 54)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Page Popularity/likes', 'Page Checkins', 'Page talking about', 'Page Category', 'Derived Feature 5', 'Derived Feature 6', 'Derived Feature 7', \n",
    "    'Derived Feature 8', 'Derived Feature 9', 'Derived Feature 10', 'Derived Feature 11', 'Derived Feature 12', 'Derived Feature 13', \n",
    "    'Derived Feature 14', 'Derived Feature 15', 'Derived Feature 16', 'Derived Feature 17', 'Derived Feature 18', 'Derived Feature 19', 'Derived Feature 20', \n",
    "    'Derived Feature 21', 'Derived Feature 22', 'Derived Feature 23', 'Derived Feature 24', 'Derived Feature 25', 'Derived Feature 26', 'Derived Feature 27', \n",
    "    'Derived Feature 28', 'Derived Feature 29', 'CC1', 'CC2', 'CC3', 'CC4', 'CC5', 'Base time', 'Post length', 'Post Share Count', 'Post Promotion Status',\n",
    "    'H Local', 'Post Published Sunday', 'Post Published Monday', 'Post Published Tuesday',  'Post Published Wednesday', 'Post Published Thursday', \n",
    "    'Post Published Friday', 'Post Published Saturday', 'Base DateTime Sunday', 'Base DateTime Monday', 'Base DateTime Tuesday','Base DateTime Wednesday', \n",
    "    'Base DateTime Thursday', 'Base DateTime Friday', 'Base DateTime Saturday', 'Target Variable' ]\n",
    "\n",
    "training1 = pd.read_csv('./Dataset/Training/Features_Variant_1.csv', sep=',', header=None, names=columns)\n",
    "training2 = pd.read_csv('./Dataset/Testing/Features_TestSet.csv', sep=',', header=None, names=columns)\n",
    "\n",
    "training_complete = pd.concat([training1, training2])\n",
    "training_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f121d1-7aaa-429a-afec-0d87ba6cd9d2",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "In this part we clean the dataset, remove columns, and adapt values for our model training.\n",
    "We dropped the post promotion status because that column is filled with all zeros.\n",
    "We also drop the duplicate rows, which are all the rows that are repeated. These rows and columns don't add any additional information as seen in the data analysis notebook.\n",
    "\n",
    "Scaling and modifying outliers was considered in other models, but in this case we didn't do scaling because decision trees are not affected by this. Also, decision trees are robust against outliers so we don't address that either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9cc4ef9-a47b-4f34-9cd1-ae7b51bcc0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training_complete.drop(\"Post Promotion Status\", axis='columns') # Drop column that has only zeros\n",
    "training = training.drop_duplicates() # drop duplicates in our dataset\n",
    "'''\n",
    "The .values changes type from pandas to a numpy array, it strips the labels of rows and columns\n",
    "'''\n",
    "X = training.iloc[:, :-1]\n",
    "y = training.iloc[:, -1]\n",
    "\n",
    "X_np = training.iloc[:, :-1].values # Independent variables - the features.\n",
    "y_np = training.iloc[:, -1].values # dependent variable - prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186f3f5-2c6e-4f33-956a-8a58ff21ae0c",
   "metadata": {},
   "source": [
    "Separate the features into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23b005b-7b11-4614-8706-0b2a478c186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size = 0.2, random_state = 1) # 20% for testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb53b8-04f2-4551-8e19-c877d5bc0645",
   "metadata": {},
   "source": [
    "## Experiements\n",
    "The next are options that were considered to train and test our model. This are the main experiements that we will be running with decision trees in which we have different hypothesis regarding the results of each different way that the model is trained. We will evaluate them, understand their result, and try to find a model with the most improved metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f8f58-49e2-4c57-87a8-f5d7309abecd",
   "metadata": {},
   "source": [
    "#### Option 1\n",
    "In here, we create a function that trains a decision tree. Based on that, it uses the function of entropy to rank the features that better divide the data. We iterate and train multiple models with the top X features to then compare results.\n",
    "This function just gets an instance of a model of decision tree, we rank the best features based on the top features that reduce entropy the most, and then use the top X features (defined by the function) to train a model. In here we are trying naive feature selection to reduce the dimensionality of our data, because some features, as seen in data analysis, are not correlated, and we want to reduce so there is less noise and see if the models improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f2b960-f991-4a2a-901d-8d815d971ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topXFeaturesDT(model_instance, X_, y_, base_model=None, feature_counts=range(5,25, 3)):\n",
    "    '''\n",
    "    Evaluates a model with different numbers of top features. We iterate on the number of features selected. \n",
    "    The 'top features' are evaluated from a decision tree model that ranks the features that caused the most information gained. \n",
    "\n",
    "    Parameters:\n",
    "    - X_: dataframe with all the values (training + testing)\n",
    "    - y_: dataframe with all target values (training + test)\n",
    "    - base_model: any decision tree model that is then used to rank the most important features\n",
    "    - feature_counts: number of top X features/columns that we are trying\n",
    "    '''\n",
    "    results = []\n",
    "\n",
    "    if base_model is None:\n",
    "        base_model = DecisionTreeRegressor()\n",
    "        base_model.fit(X_, y_)\n",
    "        \n",
    "    # Ranks most important features based on prev model\n",
    "    feature_importance = pd.DataFrame({\n",
    "    'Feature': X_.columns,\n",
    "    'Importance': base_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    X_train_full, X_test_full, y_train_top, y_test_top = train_test_split(X_, y_, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Select the x most important features\n",
    "    for x in feature_counts:\n",
    "        print(f\"\\n--- Model with top {x} features ---\")\n",
    "        top_X_features = feature_importance['Feature'].head(x).tolist()\n",
    "        \n",
    "        X_top_train = X_train_full[top_X_features]\n",
    "        X_top_test = X_test_full[top_X_features]\n",
    "\n",
    "        model = clone(model_instance)\n",
    "        model.fit(X_top_train, y_train_top)\n",
    "    \n",
    "        test(model, X_top_test, y_test_top)\n",
    "        results.append({\n",
    "            'feature_count': x,\n",
    "            'features': top_X_features,\n",
    "            'model': model,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac5cd7-c4af-45ab-a4af-392adbb130a8",
   "metadata": {},
   "source": [
    "#### Option 2\n",
    "Similar to the option 1 before, but in here the number of features to train are fixed at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43833ff3-7a4e-4c4f-ae50-9bbb53a2cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topXFeaturesDTStatic(model_instance, X_, y_, num_features, base_model=None, columns=None):\n",
    "    '''\n",
    "    Evaluates a model with top num_features passed.\n",
    "\n",
    "    Parameters:\n",
    "    - X_: dataframe with all the values (training + testing)\n",
    "    - y_: dataframe with all target values (training + test)\n",
    "    - base_model: any decision tree model that is then used to rank the most important features\n",
    "    - num_features: the number of features that will be used to train the model\n",
    "    - feature_counts: number of top X features/columns that we are trying\n",
    "    '''\n",
    "\n",
    "    X_train_full, X_test_full, y_train_top, y_test_top = train_test_split(X_, y_, test_size=0.2)\n",
    "\n",
    "    if base_model is None:\n",
    "        base_model = DecisionTreeRegressor()\n",
    "        base_model.fit(X_train_full, y_train_top)\n",
    "        \n",
    "    # Ranks most important features based on prev model\n",
    "    feature_importance = pd.DataFrame({\n",
    "    'Feature': X_.columns,\n",
    "    'Importance': base_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Select the x most important features\n",
    "    print(f\"\\n--- Model with top {num_features} features ---\")\n",
    "    top_X_features = feature_importance['Feature'].head(num_features).tolist()\n",
    "    if columns is not None:\n",
    "        top_X_features = columns\n",
    "    \n",
    "    X_top_train = X_train_full[top_X_features]\n",
    "    X_top_test = X_test_full[top_X_features]\n",
    "\n",
    "    model = clone(model_instance)\n",
    "    model.fit(X_top_train, y_train_top)\n",
    "\n",
    "    test(model, X_top_test, y_test_top)\n",
    "    results = {\n",
    "        'feature_count': num_features,\n",
    "        'features': top_X_features,\n",
    "        'model': model,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06191364-fa0f-45fb-9657-e1f6edbc47a0",
   "metadata": {},
   "source": [
    "### Option 3\n",
    "\n",
    "When data is linearly separable, linear models like linear regression, logistic regression, or linear SVMs can more effectively separate the classes or predict values. We will try to improve the linear separability of our data to enhance model performance.\n",
    "\n",
    "This approach is typically used for linear models like linear regression. However, we will also try it with decision trees to see how it performs. We're implementing this because many features in our dataset are uncorrelated with our target variable as shown in the data analysis notebooks. By applying PCA, we hope to see improved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c486d3-6e45-4a56-a826-97d891df29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_PCA(model_instance, X_, y_):\n",
    "    '''\n",
    "    The function receives a model and features/target variables and trains the model after applying PCA to the features\n",
    "\n",
    "    Parameters: \n",
    "    - model_instance: model to train\n",
    "    - X_: feautres\n",
    "    - y_: target variables\n",
    "    '''\n",
    "    # 1. Split data\n",
    "    X_train_full, X_test_full, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 2. Standardize features FIRST\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "    X_test_scaled = scaler.transform(X_test_full)\n",
    "    \n",
    "    # 3. THEN apply PCA to scaled data\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # 4. Train model (PCA output doesn't need further scaling)\n",
    "    model = clone(model_instance)\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    test(model, X_test_pca, y_test)\n",
    "    \n",
    "    results = {\n",
    "        'pca_components': X_train_pca.shape[1],\n",
    "        'model': model,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ce730-9a90-418b-84f9-f1a6a071ffd7",
   "metadata": {},
   "source": [
    "## Test - Creating functions\n",
    "The next function have the function to be used to test the model with various metrics and compare the different models.\n",
    "\n",
    "Explanation of metrics used:\n",
    "- rmse (inf): average magnitude of prediction errors, but in the same units as your target variable. penalizes more big errors. i.e. penalizes more being\n",
    "- 100 comments off than 50 comments from real value\n",
    "- mae (inf): average absolute difference between predicted and real comment value\n",
    "- r2 (0-1): compares model error against as if you predicted average values. 0.65 means 65% of the variance in comment counts. aiming >0.7\n",
    "- mabse (inf): median abolsute error orders all errors and returns the median/middle value. This helps when values are skewed\n",
    "- smape(0-200): percentage of: average of (absolute difference between y_pred and y_test) / (average value of y_pred,y_test)\n",
    "\n",
    "These metrics give us a comprehensive view of model performance, with each metric highlighting different aspects of prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b755f2c4-6a93-4ffd-b2db-f4799dc12d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smape(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate the smape which is the percentage of: average of (absolute difference between y_pred and y_test) / (average value of y_pred,y_test)\n",
    "\n",
    "    Params:\n",
    "    y_true: the real values of the dataset\n",
    "    y_pred: the predictions of our model\n",
    "    '''\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    \n",
    "    zero_indexes = denominator == 0\n",
    "    \n",
    "    valid_entries = ~zero_indexes\n",
    "    if not np.any(valid_entries):\n",
    "        return 0.0\n",
    "    \n",
    "    smape_values = 2 * numerator[valid_entries] / denominator[valid_entries] * 100\n",
    "    return np.mean(smape_values)\n",
    "    \n",
    "def test(model, X_test_=X_test, y_test_=y_test, y_pred=None):\n",
    "    '''\n",
    "    We test our model and print various metrics for comparison\n",
    "\n",
    "    Params:\n",
    "    model: to test\n",
    "    X_test: which are features to test\n",
    "    y_test: the real values that match X_test\n",
    "    '''\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test_)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_test_, y_pred)\n",
    "    mae = mean_absolute_error(y_test_, y_pred)\n",
    "    mse = mean_squared_error(y_test_, y_pred)\n",
    "    mabse = median_absolute_error(y_test_, y_pred)\n",
    "    r2 = r2_score(y_test_, y_pred)\n",
    "    smape = calculate_smape(y_test_, y_pred)\n",
    "\n",
    "    print(f\"Root mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean absolute Error: {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Median absolute Error: {mabse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"sMAPE: {smape:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744bcdd-6f9d-46b8-9bab-14fafb3fd089",
   "metadata": {},
   "source": [
    "## Training\n",
    "In this part, we train the model and for each model we evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c398f7-40df-4359-90de-2f7f250f29ca",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "#### Model with all the features\n",
    "This is the baseline model using all available features. After very simple data preprocessing, we compute the testing values to evaluate performance. This will be our baseline model which we aim to improve upon through iteration. We believe decision tree is a good choice because it is robust against outliers which is one of the main problems our dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b568f15-eb77-4fdc-b283-2c2e0ff313eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared Error: 53.9661\n",
      "Mean absolute Error: 9.2336\n",
      "Mean Squared Error: 2912.35\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.0546\n",
      "sMAPE: 116.3375\n"
     ]
    }
   ],
   "source": [
    "dt_regressor = DecisionTreeRegressor()\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "test(dt_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699719d-08ca-466d-80ed-d81e00c97db3",
   "metadata": {},
   "source": [
    "#### Decision tree Regressor with top X features\n",
    "After establishing our baseline model, the next code iterates through the same model but with different feature subsets. The function selects the best features (those that provide the most information gain) and uses the top X features to evaluate the model. We do this to see if there is any improvement, as we expect the model to be more accurate with fewer features since many are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3cd15967-4ad1-4be0-b74d-d168fe499ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model with top 5 features ---\n",
      "Root mean Squared Error: 58.5504\n",
      "Mean absolute Error: 10.5416\n",
      "Mean Squared Error: 3428.15\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.3011\n",
      "sMAPE: 122.7723\n",
      "\n",
      "--- Model with top 8 features ---\n",
      "Root mean Squared Error: 61.6623\n",
      "Mean absolute Error: 10.0507\n",
      "Mean Squared Error: 3802.24\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.2249\n",
      "sMAPE: 116.9483\n",
      "\n",
      "--- Model with top 11 features ---\n",
      "Root mean Squared Error: 64.0536\n",
      "Mean absolute Error: 9.9791\n",
      "Mean Squared Error: 4102.87\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.1636\n",
      "sMAPE: 116.9760\n",
      "\n",
      "--- Model with top 14 features ---\n",
      "Root mean Squared Error: 58.5225\n",
      "Mean absolute Error: 9.2960\n",
      "Mean Squared Error: 3424.89\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.3018\n",
      "sMAPE: 116.6560\n",
      "\n",
      "--- Model with top 17 features ---\n",
      "Root mean Squared Error: 62.5704\n",
      "Mean absolute Error: 9.8812\n",
      "Mean Squared Error: 3915.06\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.2019\n",
      "sMAPE: 116.5879\n",
      "\n",
      "--- Model with top 20 features ---\n",
      "Root mean Squared Error: 64.2587\n",
      "Mean absolute Error: 10.2121\n",
      "Mean Squared Error: 4129.17\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.1582\n",
      "sMAPE: 117.3292\n",
      "\n",
      "--- Model with top 23 features ---\n",
      "Root mean Squared Error: 65.3026\n",
      "Mean absolute Error: 10.0605\n",
      "Mean Squared Error: 4264.43\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.1307\n",
      "sMAPE: 116.8576\n"
     ]
    }
   ],
   "source": [
    "dt_regressor_top = DecisionTreeRegressor()\n",
    "dt_regressor_top = topXFeaturesDTRegressor(dt_regressor_top, X, y, dt_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e6e73-5aa3-48cd-9d69-3cfc6588a05f",
   "metadata": {},
   "source": [
    "#### Results\n",
    "As seen in the metrics, the measure that improved most was the R² score when using the top 14 features, which means the model is capturing a better proportion of the variance in the target variable.\n",
    "The best results achieved for the plain decision tree regressor is the model with the top 14 features, with an R² Score of 0.3018, which is a significant improvement from the previous model's score of 0.05. This could be due to the reduction of the number of features - many irrelevant features that caused noise are removed, improving the overall metrics of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf8a12-7f3a-466f-94cc-db68d7c1a261",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "As seen in class, we train the random forest (bagging) which builds many decision trees and then predicts based on the average of all individual predictions. This reduces variance in our model. We apply the same experiments as before, training with all the data and then with only the top features.\n",
    "\n",
    "Random forests are valuable for this dataset because they combine the robustness of decision trees with improved generalization through ensemble learning. By creating multiple trees with different subsets of data and features, random forests can better handle the complex relationships in our Facebook comment prediction task while being less prone to overfitting than individual decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632bb85-cb90-49fb-89f2-9848e69899b5",
   "metadata": {},
   "source": [
    "We train with all the data available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "77d9fa41-abb7-46ae-a6f1-d74e70cd3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared Error: 40.2087\n",
      "Mean absolute Error: 7.2797\n",
      "Mean Squared Error: 1616.74\n",
      "Median absolute Error: 0.7700\n",
      "R² Score: 0.4752\n",
      "sMAPE: 130.1362\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_features='sqrt',  # Try sqrt(n_features) at each split\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "test(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb1823-3e2c-4309-a8ea-adde57f11fe4",
   "metadata": {},
   "source": [
    "We now train a random forest regressor but with the top X features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7d7bc40c-d60f-439d-a951-6f3786c1b739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model with top 5 features ---\n",
      "Root mean Squared Error: 48.4196\n",
      "Mean absolute Error: 8.0247\n",
      "Mean Squared Error: 2344.46\n",
      "Median absolute Error: 0.7100\n",
      "R² Score: 0.5221\n",
      "sMAPE: 132.7389\n",
      "\n",
      "--- Model with top 8 features ---\n",
      "Root mean Squared Error: 46.1015\n",
      "Mean absolute Error: 7.3203\n",
      "Mean Squared Error: 2125.35\n",
      "Median absolute Error: 0.7200\n",
      "R² Score: 0.5667\n",
      "sMAPE: 131.0601\n",
      "\n",
      "--- Model with top 11 features ---\n",
      "Root mean Squared Error: 46.9869\n",
      "Mean absolute Error: 7.3963\n",
      "Mean Squared Error: 2207.76\n",
      "Median absolute Error: 0.7000\n",
      "R² Score: 0.5499\n",
      "sMAPE: 130.2246\n",
      "\n",
      "--- Model with top 14 features ---\n",
      "Root mean Squared Error: 47.0395\n",
      "Mean absolute Error: 7.3805\n",
      "Mean Squared Error: 2212.72\n",
      "Median absolute Error: 0.7100\n",
      "R² Score: 0.5489\n",
      "sMAPE: 129.5320\n",
      "\n",
      "--- Model with top 17 features ---\n",
      "Root mean Squared Error: 46.9154\n",
      "Mean absolute Error: 7.3760\n",
      "Mean Squared Error: 2201.05\n",
      "Median absolute Error: 0.7000\n",
      "R² Score: 0.5513\n",
      "sMAPE: 129.8292\n",
      "\n",
      "--- Model with top 20 features ---\n",
      "Root mean Squared Error: 47.3878\n",
      "Mean absolute Error: 7.5156\n",
      "Mean Squared Error: 2245.60\n",
      "Median absolute Error: 0.7300\n",
      "R² Score: 0.5422\n",
      "sMAPE: 129.7936\n",
      "\n",
      "--- Model with top 23 features ---\n",
      "Root mean Squared Error: 48.2398\n",
      "Mean absolute Error: 7.6226\n",
      "Mean Squared Error: 2327.08\n",
      "Median absolute Error: 0.7500\n",
      "R² Score: 0.5256\n",
      "sMAPE: 130.3140\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_features='sqrt',  # Try sqrt(n_features) at each split\n",
    "    random_state=42\n",
    ")\n",
    "dt_regressor_top = topXFeaturesDTRegressor(rf_model, X, y, dt_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766000b-803c-434c-83f1-7e4556ffb6a2",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Overall, when using the Random Forest regressor, models with fewer features (especially with the top 5-17 features) have:\n",
    "\n",
    "Higher RMSE and MSE (which normally indicates worse performance)\n",
    "But higher R² scores (which indicates better performance)\n",
    "\n",
    "We get higher MSE but lower mean absolute error overall. We believe this is because we are reducing the variance and we are doing worse at predicting outliers with big amount of comments, which are causing the MSE to increase. However, the median absolute error orders the errors and picks the one in the middle, which is better when we have high skewness, which in our case we do (many instances have very low comment numbers with few having high numbers). Thus, we can see that we are doing worse when the post has high amount of comments but better with the overall posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216d159-0c4c-4e31-b3f6-4a723a9555b2",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "XGBoost is an optimized implementation of boosting (algorithm seen in class), where it builds an ensemble method to reduce bias. We believe that we could improve our results more than with random forest by using boosting as our dataset, because it is very dispersed, could need a more complex model, thus we need to reduce bias. Even more so, XGBoost includes regularization parameters to prevent variance from increasing.\n",
    "\n",
    "The max_depth (3) helps prevent overfitting on outlier comment counts, while the learning_rate (0.1) ensures steady improvement without missing optimal solutions. For our Facebook comment prediction task, these settings balance the need for a complex enough model to capture the relationship between post features and comment volume, while avoiding overfitting to the highly skewed distribution of comments. We will use these results for comparison against other hyperparameter tunning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f01128-fb40-4306-81e3-57ef09af28d0",
   "metadata": {},
   "source": [
    "We train the model with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "960e561e-428f-416a-90d3-44f74d0c6fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared Error: 37.9187\n",
      "Mean absolute Error: 7.5025\n",
      "Mean Squared Error: 1437.83\n",
      "Median absolute Error: 0.9971\n",
      "R² Score: 0.5332\n",
      "sMAPE: 144.9285\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "test(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f9c67-f45b-450b-b89c-35e589073b7e",
   "metadata": {},
   "source": [
    "Then we train the model with only the top X features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6110b0b9-0875-49bc-a1cf-f95757de40bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model with top 5 features ---\n",
      "Root mean Squared Error: 49.8174\n",
      "Mean absolute Error: 8.4805\n",
      "Mean Squared Error: 2481.77\n",
      "Median absolute Error: 0.9353\n",
      "R² Score: 0.4941\n",
      "sMAPE: 145.2685\n",
      "\n",
      "--- Model with top 8 features ---\n",
      "Root mean Squared Error: 45.7413\n",
      "Mean absolute Error: 7.9815\n",
      "Mean Squared Error: 2092.27\n",
      "Median absolute Error: 0.7826\n",
      "R² Score: 0.5735\n",
      "sMAPE: 144.2159\n",
      "\n",
      "--- Model with top 11 features ---\n",
      "Root mean Squared Error: 46.3077\n",
      "Mean absolute Error: 7.9661\n",
      "Mean Squared Error: 2144.41\n",
      "Median absolute Error: 0.9148\n",
      "R² Score: 0.5628\n",
      "sMAPE: 142.6087\n",
      "\n",
      "--- Model with top 14 features ---\n",
      "Root mean Squared Error: 46.6957\n",
      "Mean absolute Error: 7.9914\n",
      "Mean Squared Error: 2180.49\n",
      "Median absolute Error: 0.8937\n",
      "R² Score: 0.5555\n",
      "sMAPE: 141.9464\n",
      "\n",
      "--- Model with top 17 features ---\n",
      "Root mean Squared Error: 46.5560\n",
      "Mean absolute Error: 7.9538\n",
      "Mean Squared Error: 2167.46\n",
      "Median absolute Error: 0.9332\n",
      "R² Score: 0.5581\n",
      "sMAPE: 141.4882\n",
      "\n",
      "--- Model with top 20 features ---\n",
      "Root mean Squared Error: 46.7910\n",
      "Mean absolute Error: 8.0178\n",
      "Mean Squared Error: 2189.40\n",
      "Median absolute Error: 0.9003\n",
      "R² Score: 0.5537\n",
      "sMAPE: 146.1055\n",
      "\n",
      "--- Model with top 23 features ---\n",
      "Root mean Squared Error: 47.2069\n",
      "Mean absolute Error: 8.0363\n",
      "Mean Squared Error: 2228.50\n",
      "Median absolute Error: 0.9046\n",
      "R² Score: 0.5457\n",
      "sMAPE: 143.9896\n"
     ]
    }
   ],
   "source": [
    "xgb_model_top = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model_top_results = topXFeaturesDT(xgb_model_top, X, y, dt_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e6f35-af39-429b-af46-4fcb952dbb58",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Similar to our findings with Random Forest, the XGBoost models with fewer features (particularly with 8 features) demonstrate interesting performance characteristics:\n",
    "\n",
    "Higher RMSE and MSE compared to the full-feature model, but significantly improved R² scores\n",
    "The model with top 8 features achieves the best R² score of 0.5735, outperforming both the full-feature XGBoost model (0.5332) and the best Random Forest model\n",
    "\n",
    "We see that overall the XGBoost is able to get better R² scores and median errors with 8 features. This again is because boosting algorithms excel at reducing bias in predictions by sequentially focusing on difficult examples. The top 8 features provide sufficient signal while eliminating noise that might confuse the model.\n",
    "The slightly higher sMAPE values across XGBoost models suggest that while it's better at capturing overall patterns (higher R2), it may sometimes make proportionally larger errors on posts with very few comments. However, the improved median absolute error indicates that for most posts, the prediction accuracy is better than with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e43aff-331f-40c1-bd50-f55e46dde85f",
   "metadata": {},
   "source": [
    "## Final model\n",
    "From previous iterations, we found that models with 7-15 features demonstrated the best overall metrics. Now, we'll train both XGBoost and RandomForest using this feature range while employing cross-validation to identify optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72768a92-01db-4c72-86e7-6cac5914ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10 # we choose top 10 number of features to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2210bd-87c9-4028-bbfe-835bc26a6ea0",
   "metadata": {},
   "source": [
    "For RandomForest, we're exploring key hyperparameters including:\n",
    "\n",
    "Number of trees (50-200)\n",
    "Maximum tree depth (None-30)\n",
    "Minimum samples required for node splitting and leaf creation\n",
    "Feature selection strategies (sqrt, log2, percentage)\n",
    "Bootstrap sampling options\n",
    "Impurity thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96af7b0-6e1c-4728-867b-fd9721ac6f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model with top 10 features ---\n",
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "Root mean Squared Error: 46.7323\n",
      "Mean absolute Error: 7.1752\n",
      "Mean Squared Error: 2183.90\n",
      "Median absolute Error: 0.6800\n",
      "R² Score: 0.5548\n",
      "sMAPE: 132.7409\n",
      "Best parameters: RandomForestRegressor(bootstrap=False, max_depth=30, max_features='sqrt',\n",
      "                      min_samples_leaf=2, min_samples_split=5, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "rf_model_grid = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# All possible hypterparameters to configure based on documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],                  # num trees\n",
    "    'max_depth': [None, 10, 20, 30],                 # set the max depth\n",
    "    'min_samples_split': [2, 5, 10],                \n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.33],    \n",
    "    'bootstrap': [True, False],                      # whether to use bootstrapping or not\n",
    "    'min_impurity_decrease': [0.0, 0.01] \n",
    "}\n",
    "\n",
    "# Create the cross-vaidation method\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model_grid,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                               # 5 fold cross validation\n",
    "    scoring='neg_mean_squared_error',   \n",
    "    n_jobs=-1,                          \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "results = topXFeaturesDTStatic(grid_search,X, y, num_features, dt_regressor)\n",
    "print(\"Best parameters:\", results['model'].best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec013c5-e12f-4f59-b74c-5f0d6fa11cc2",
   "metadata": {},
   "source": [
    "For XGBoost, we're tuning:\n",
    "\n",
    "Number of boosting rounds (50-200)\n",
    "Learning rate (0.01-0.3)\n",
    "Tree depth and complexity controls\n",
    "Sampling parameters for observations and features\n",
    "Regularization parameters (alpha and lambda) to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0069af5a-e1f9-4415-8143-63dbc6f37f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_grid = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# All possible hypterparameters to configure based on documentation: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],        \n",
    "    'learning_rate': [0.01, 0.1, 0.3],      \n",
    "    'max_depth': [3, 5, 7],                  # Depth of trees\n",
    "    'min_child_weight': [1, 3, 5],           \n",
    "    'gamma': [0, 0.1, 0.3],           \n",
    "    'subsample': [0.7, 0.8, 1.0],           \n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],  \n",
    "    # regularization to penalize large values. mentioned it we saw it in class\n",
    "    'reg_alpha': [0, 0.1, 1],                \n",
    "    'reg_lambda': [0.1, 1, 10]             \n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search_xg = GridSearchCV(\n",
    "    estimator=xgb_model_grid,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "#dimensionality reduction\n",
    "results = topXFeaturesDTStatic(grid_search_xg,X, y, num_features, dt_regressor)\n",
    "print(\"Best parameters:\", results['model'].best_estimator_)\n",
    "\n",
    "# Saved the model\n",
    "# from joblib import dump, load\n",
    "# dump(results['model'], f'model_xgboost.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed814f5-da7a-4482-8bcc-a750bc310e4c",
   "metadata": {},
   "source": [
    "For repeating the previous metrics, you can run the following code to train the model and see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc17cc0-70c1-4d52-86e6-94cd025b65b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model with top 10 features ---\n",
      "Root mean Squared Error: 25.8804\n",
      "Mean absolute Error: 5.5586\n",
      "Mean Squared Error: 669.79\n",
      "Median absolute Error: 0.8810\n",
      "R² Score: 0.8040\n",
      "sMAPE: 137.4270\n"
     ]
    }
   ],
   "source": [
    "saved_cross_validation = load('./Models/model_xgboost.joblib')\n",
    "\n",
    "model_columns = ['CC2','Base time','Post Share Count','Derived Feature 25','H Local','Post length','Derived Feature 14','Page Popularity/likes',\n",
    "                'Derived Feature 22','Derived Feature 7']\n",
    "# Get the best parameters from saved grid search\n",
    "best_params = saved_cross_validation.best_estimator_.get_params()\n",
    "\n",
    "# Create new model with these parameters\n",
    "best_model = xgb.XGBRegressor(**best_params)\n",
    "num_features = 10\n",
    "results = topXFeaturesDTStatic(best_model, X, y, num_features, columns=model_columns)\n",
    "model = results['model']\n",
    "\n",
    "X_filtered = X[model_columns]\n",
    "X_train_temp, X_test_temp, y_train_temp, y_test = train_test_split(X_filtered, y_np, test_size = 0.2) # 20% for testing dataset\n",
    "test(model, X_test_temp, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6cf25d-133d-41a1-9194-574a6e226fc3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "After comprehensive experimentation with feature selection and hyperparameter tuning, our key findings include:\n",
    "\n",
    "XGBoost significantly outperforms Random Forest: Our optimized XGBoost model achieved an R² score of 0.8040, explaining 80% of the variance in comment volumes - a substantial improvement over both baseline models and optimized Random Forest (R² of 0.5548).\n",
    "Feature reduction improved performance: Using only the top 8-10 features consistently outperformed using the full feature set, confirming our hypothesis that many features were irrelevant or noisy.\n",
    "Error metrics trade-offs: XGBoost showed better RMSE, MAE, and MSE, while Random Forest had slightly better median absolute error, suggesting XGBoost handles the full range of values better.\n",
    "Bias-variance explanation: XGBoost's superior performance indicates our problem needed bias reduction more than variance reduction, aligning with the complex relationships in our dataset.\n",
    "\n",
    "Based on these findings, we select the optimized XGBoost model with the top 10 features as our best model, demonstrating strong predictive capability for Facebook comment volume prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87a0a4-9c3c-41ae-9c01-b8c6f2097ed8",
   "metadata": {},
   "source": [
    "## Trying to improve linear separability\n",
    "For our final experiment, we applied PCA to our dataset before training with our best XGBoost model (identified through previous cross-validation). PCA transforms the data into uncorrelated components that might improve linear separability.\n",
    "\n",
    "As seen in the results below, the PCA transformation significantly decreased performance compared to our optimized XGBoost model (R² dropped from 0.8040 to 0.3152). This suggests that the non-linear relationships captured by XGBoost on the original features are lost when transforming to principal components. While PCA creates orthogonal features that might benefit linear models, our best-performing model (XGBoost) already handles feature interactions effectively through tree-based structures. The results confirm that for this Facebook comment prediction task, feature selection is a more effective dimensionality reduction approach than PCA transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a05cfbb-a31a-400c-b133-2e94a572405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean Squared Error: 57.9580\n",
      "Mean absolute Error: 10.9336\n",
      "Mean Squared Error: 3359.12\n",
      "Median absolute Error: 1.7670\n",
      "R² Score: 0.3152\n",
      "sMAPE: 147.5719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_cross_validation = load('model_xgboost.joblib')\n",
    "\n",
    "# Get the best parameters from saved grid search\n",
    "best_params = saved_cross_validation.best_estimator_.get_params()\n",
    "\n",
    "# Create new model with these parameters\n",
    "best_model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "results = DT_PCA(best_model, X, y)\n",
    "results['pca_components']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afda20-cb87-4d95-905f-83493144f284",
   "metadata": {},
   "source": [
    "# Extra features\n",
    "For this last experiment, we will use this option after finding the optimal model using the normal dataset. This option contains all the features created in the notebook featureEngineering.ipynb. It creates new features that provide more valuable information. All this data will be used to train the best decision tree obtained using all the other experiments to see if there is any improvement.\n",
    "\n",
    "For better understanding of these features, more explanation is provided on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "beba753c-6e31-4adb-8952-a1f8886af159",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns='Target_Comment_Volume')\n",
    "y_train = train_df['Target_Comment_Volume']\n",
    "\n",
    "X_test  = test_df.drop(columns='Target_Comment_Volume')\n",
    "y_test  = test_df['Target_Comment_Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6f9449e-ce36-490a-9601-5469d15254b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---Train model with all the features--->\n",
      "Root mean Squared Error: 14.6582\n",
      "Mean absolute Error: 3.3010\n",
      "Mean Squared Error: 214.86\n",
      "Median absolute Error: 0.5319\n",
      "R² Score: 0.7879\n",
      "sMAPE: 140.6487\n",
      "\n",
      "--- Model with top 10 features ---\n",
      "Root mean Squared Error: 18.6941\n",
      "Mean absolute Error: 3.6789\n",
      "Mean Squared Error: 349.47\n",
      "Median absolute Error: 0.5449\n",
      "R² Score: 0.6854\n",
      "sMAPE: 141.5594\n"
     ]
    }
   ],
   "source": [
    "saved_cross_validation = load('./Models/model_xgboost.joblib')\n",
    "\n",
    "# Get the best parameters from saved grid search\n",
    "best_params = saved_cross_validation.best_estimator_.get_params()\n",
    "\n",
    "# use one-hot encoding\n",
    "categorical_cols = ['Page_Category', 'Post_Promotion_Status', 'Published_Day', 'BaseDate_Day']\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols)\n",
    "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# Train model with all the features\n",
    "print(\"<---Train model with all the features--->\")\n",
    "best_model = xgb.XGBRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "test(best_model, X_test, y_test)\n",
    "\n",
    "X = pd.concat([X_train, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "best_model1 = xgb.XGBRegressor(**best_params)\n",
    "num_features = 10\n",
    "results = topXFeaturesDTStatic(best_model1, X, y, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45fa66-568d-4e4d-afdd-84619b23cf19",
   "metadata": {},
   "source": [
    "### Conclusion extra feature engineering\n",
    "After evaluating the impact of feature engineering on our model, the results show improvements in most performance metrics. Using the new features derived from the original dataset in featureEngineering.ipynb, our best XGBoost model achieved an R2 score of 0.7879, which is slightly lower than our previous best of 0.8040. However, other metrics showed improvements: RMSE decreased from 25.88 to 14.66, MSE dropped from 669.79 to 214.86, and MAE improved from 5.56 to 3.30.\n",
    "\n",
    "This suggests that our engineered features (hourly rates, category-level target means, and ratio aggregates) particularly enhanced the model's ability to make accurate predictions across the entire dataset, especially for posts with typical comment volumes or when posts deactivated comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf01e0-b679-4807-bd73-f462cac4075e",
   "metadata": {},
   "source": [
    "# Final conclusion\n",
    "In this project, we aimed to predict the number of comments a Facebook post will receive using various decision tree regression models. We conducted several experiments to understand the data and improve prediction accuracy:\n",
    "1. Baseline Model: Decision Tree Regressor\n",
    "2. Random Forest with all and top features. Best results with top 14 features\n",
    "3. XGBoost with all and top features. Best results with top 8 features. \n",
    "\n",
    "## Hyperparameter Tuning with Cross-Validation\n",
    "We conducted extensive hyperparameter tuning to optimize our most promising models:\n",
    "\n",
    "### RandomForest with Cross-Validation\n",
    "Used GridSearchCV with 5-fold cross-validation\n",
    "Tested 6,480 hyperparameter combinations\n",
    "Best results:\n",
    "Root mean Squared Error: 46.7323\n",
    "Mean absolute Error: 7.1752\n",
    "Mean Squared Error: 2183.90\n",
    "Median absolute Error: 0.6800\n",
    "R² Score: 0.5548\n",
    "sMAPE: 132.7409\n",
    "\n",
    "### XGBoost with Cross-Validation\n",
    "\n",
    "Similar extensive hyperparameter search\n",
    "Selected the optimal hyperparameters for our dataset\n",
    "Best results: \n",
    "Root mean Squared Error: 25.8804\n",
    "Mean absolute Error: 5.5586\n",
    "Mean Squared Error: 669.79\n",
    "Median absolute Error: 0.8810\n",
    "R² Score: 0.8040\n",
    "sMAPE: 137.4270\n",
    "\n",
    "## PCA Experiment for Linear Separability\n",
    "As a final experiment, we applied PCA to our dataset before training with our best XGBoost model, where performance declined significantly in all metrics.\n",
    "\n",
    "## Extra Feature Engineering\n",
    "As a final experiment, we created domain-specific features including hourly comment rates, category-level target encoding, and post engagement indicators, which significantly improved prediction accuracy for typical posts while maintaining comparable R² scores.\n",
    "\n",
    "## Final model and insights\n",
    "Our best model is the XGBoost regressor with all the extra feature engieering and optimized hyperparameters through cross-validation. From all our experiments, we concluded that feature selection is crucial as eliminating noise is very important. We also found that with this model, bias reduction was more important than variance reduction as me saw that having higher compelx model got better results that reducing variance (when we compared boosting vs random forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf4c1b-faeb-46fd-a711-0459aeb38cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
