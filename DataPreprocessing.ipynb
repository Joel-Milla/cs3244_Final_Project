{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf0cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc7d52-ce3e-4341-9ba8-70093108f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Page Popularity/likes', 'Page Checkins', 'Page talking about', 'Page Category', 'Derived Feature 5', 'Derived Feature 6', 'Derived Feature 7', \n",
    "    'Derived Feature 8', 'Derived Feature 9', 'Derived Feature 10', 'Derived Feature 11', 'Derived Feature 12', 'Derived Feature 13', \n",
    "    'Derived Feature 14', 'Derived Feature 15', 'Derived Feature 16', 'Derived Feature 17', 'Derived Feature 18', 'Derived Feature 19', 'Derived Feature 20', \n",
    "    'Derived Feature 21', 'Derived Feature 22', 'Derived Feature 23', 'Derived Feature 24', 'Derived Feature 25', 'Derived Feature 26', 'Derived Feature 27', \n",
    "    'Derived Feature 28', 'Derived Feature 29', 'CC1', 'CC2', 'CC3', 'CC4', 'CC5', 'Base time', 'Post length', 'Post Share Count', 'Post Promotion Status',\n",
    "    'H Local', 'Post Published Sunday', 'Post Published Monday', 'Post Published Tuesday',  'Post Published Wednesday', 'Post Published Thursday', \n",
    "    'Post Published Friday', 'Post Published Saturday', 'Base DateTime Sunday', 'Base DateTime Monday', 'Base DateTime Tuesday','Base DateTime Wednesday', \n",
    "    'Base DateTime Thursday', 'Base DateTime Friday', 'Base DateTime Saturday', 'Target Variable' ]\n",
    "\n",
    "data = pd.read_csv('./Dataset/Training/Features_Variant_1.csv', sep=',', header=None, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b7b73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, y_test, y_pred=None):\n",
    "    '''\n",
    "    We test our model and print various metrics for comparison\n",
    "\n",
    "    Params:\n",
    "    model: to test\n",
    "    X_test: which are features to test\n",
    "    y_test: the real values that match X_test\n",
    "    '''\n",
    "    if y_pred is None:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mabse = median_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Root mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"Mean absolute Error: {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"Median absolute Error: {mabse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acef4df",
   "metadata": {},
   "source": [
    "We firstly try without any preprocessing in order to see that it can be useful or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910ffcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Root mean Squared Error: 27.8424\n",
      "Mean absolute Error: 6.3344\n",
      "Mean Squared Error: 775.20\n",
      "Median absolute Error: 0.8000\n",
      "R² Score: 0.2366\n",
      "Decision Tree\n",
      "Root mean Squared Error: 30.8626\n",
      "Mean absolute Error: 5.5978\n",
      "Mean Squared Error: 952.50\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.0620\n",
      "Linear Regression\n",
      "Root mean Squared Error: 25.8992\n",
      "Mean absolute Error: 8.2284\n",
      "Mean Squared Error: 670.77\n",
      "Median absolute Error: 4.3224\n",
      "R² Score: 0.3395\n",
      "Random Forest\n",
      "Root mean Squared Error: 19.3486\n",
      "Mean absolute Error: 4.0103\n",
      "Mean Squared Error: 374.37\n",
      "Median absolute Error: 0.5400\n",
      "R² Score: 0.6313\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"KNN\")\n",
    "test(knn, X_test, y_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"Decision Tree\")\n",
    "test(dt, X_test, y_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Linear Regression\")\n",
    "test(lr, X_test, y_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Random Forest\")\n",
    "test(rf, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b92bc-1c30-4d9e-b688-96aaa80ed368",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153949d",
   "metadata": {},
   "source": [
    " ### Why Use a Deep Autoencoder?\n",
    "\n",
    "The purpose of using a deep autoencoder in this project is to perform automatic feature compression before feeding the data into regression models.\n",
    "\n",
    "**Why Autoencoders?**\n",
    "- High-dimensional feature sets may contain noise and redundant information.\n",
    "- Autoencoders can learn non-linear relationships and compress the data into a lower-dimensional representation.\n",
    "- This can reduce overfitting and improve generalization, especially for distance-based models like KNN.\n",
    "\n",
    "**What we do here:**\n",
    "- We experiment with various encoder output dimensions: 8, 16, and 32.\n",
    "- We try different layer sizes (64 and 128) to understand which architecture gives the best downstream performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30862c",
   "metadata": {},
   "source": [
    "Dim 8 Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1821138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 22.7580\n",
      "Mean absolute Error: 5.0522\n",
      "Mean Squared Error: 517.93\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.4642\n",
      "<----Decision tree after autoencoder---->\n",
      "Root mean Squared Error: 31.0678\n",
      "Mean absolute Error: 6.6559\n",
      "Mean Squared Error: 965.21\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.0014\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 24.2972\n",
      "Mean absolute Error: 12.1064\n",
      "Mean Squared Error: 590.36\n",
      "Median absolute Error: 9.0747\n",
      "R² Score: 0.3893\n",
      "<----Random Forest after autoencoder---->\n",
      "Root mean Squared Error: 20.8889\n",
      "Mean absolute Error: 4.9870\n",
      "Mean Squared Error: 436.35\n",
      "Median absolute Error: 0.7200\n",
      "R² Score: 0.5486\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "# Standardize features before autoencoding\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 8 # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "test(knn, X_test_enc, y_test_enc)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Decision tree after autoencoder---->\")\n",
    "test(dt, X_test_enc, y_test_enc)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "test(lr, X_test_enc, y_test_enc)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Random Forest after autoencoder---->\")\n",
    "test(rf, X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2bf45",
   "metadata": {},
   "source": [
    "Dim 16 Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc9496ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 21.3925\n",
      "Mean absolute Error: 4.7361\n",
      "Mean Squared Error: 457.64\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.5266\n",
      "<----Decision tree after autoencoder---->\n",
      "Root mean Squared Error: 36.0676\n",
      "Mean absolute Error: 6.9967\n",
      "Mean Squared Error: 1300.87\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: -0.3458\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 21.9840\n",
      "Mean absolute Error: 9.0810\n",
      "Mean Squared Error: 483.30\n",
      "Median absolute Error: 5.7983\n",
      "R² Score: 0.5000\n",
      "<----Random Forest after autoencoder---->\n",
      "Root mean Squared Error: 21.0502\n",
      "Mean absolute Error: 4.9027\n",
      "Mean Squared Error: 443.11\n",
      "Median absolute Error: 0.7100\n",
      "R² Score: 0.5416\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "# Standardize features before autoencoding\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 16 # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "test(knn, X_test_enc, y_test_enc)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Decision tree after autoencoder---->\")\n",
    "test(dt, X_test_enc, y_test_enc)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "test(lr, X_test_enc, y_test_enc)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Random Forest after autoencoder---->\")\n",
    "test(rf, X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7de16",
   "metadata": {},
   "source": [
    "Dim 32 Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d62dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 22.1247\n",
      "Mean absolute Error: 4.7540\n",
      "Mean Squared Error: 489.50\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.4936\n",
      "<----Decision tree after autoencoder---->\n",
      "Root mean Squared Error: 32.4660\n",
      "Mean absolute Error: 6.1632\n",
      "Mean Squared Error: 1054.04\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: -0.0904\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 21.1119\n",
      "Mean absolute Error: 5.3770\n",
      "Mean Squared Error: 445.71\n",
      "Median absolute Error: 1.9281\n",
      "R² Score: 0.5389\n",
      "<----Random Forest after autoencoder---->\n",
      "Root mean Squared Error: 21.0156\n",
      "Mean absolute Error: 4.7940\n",
      "Mean Squared Error: 441.65\n",
      "Median absolute Error: 0.6900\n",
      "R² Score: 0.5431\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "# Standardize features before autoencoding\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 32 # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "test(knn, X_test_enc, y_test_enc)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Decision tree after autoencoder---->\")\n",
    "test(dt, X_test_enc, y_test_enc)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "test(lr, X_test_enc, y_test_enc)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Random Forest after autoencoder---->\")\n",
    "test(rf, X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec05e74",
   "metadata": {},
   "source": [
    "Dim 8 Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2baf14b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 22.2540\n",
      "Mean absolute Error: 4.9397\n",
      "Mean Squared Error: 495.24\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.4877\n",
      "<----Decision tree after autoencoder---->\n",
      "Root mean Squared Error: 34.2901\n",
      "Mean absolute Error: 6.8371\n",
      "Mean Squared Error: 1175.81\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: -0.2164\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 22.8160\n",
      "Mean absolute Error: 11.2708\n",
      "Mean Squared Error: 520.57\n",
      "Median absolute Error: 8.6747\n",
      "R² Score: 0.4614\n",
      "<----Random Forest after autoencoder---->\n",
      "Root mean Squared Error: 21.3678\n",
      "Mean absolute Error: 4.9097\n",
      "Mean Squared Error: 456.58\n",
      "Median absolute Error: 0.7400\n",
      "R² Score: 0.5276\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "# Standardize features before autoencoding\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 8 # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "test(knn, X_test_enc, y_test_enc)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Decision tree after autoencoder---->\")\n",
    "test(dt, X_test_enc, y_test_enc)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "test(lr, X_test_enc, y_test_enc)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Random Forest after autoencoder---->\")\n",
    "test(rf, X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c1fd4",
   "metadata": {},
   "source": [
    "Dim 16 Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b9dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 21.8257\n",
      "Mean absolute Error: 4.7603\n",
      "Mean Squared Error: 476.36\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.5072\n",
      "<----Decision tree after autoencoder---->\n",
      "Root mean Squared Error: 33.0705\n",
      "Mean absolute Error: 6.7394\n",
      "Mean Squared Error: 1093.65\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: -0.1314\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 22.5850\n",
      "Mean absolute Error: 10.6199\n",
      "Mean Squared Error: 510.08\n",
      "Median absolute Error: 6.9628\n",
      "R² Score: 0.4723\n",
      "<----Random Forest after autoencoder---->\n",
      "Root mean Squared Error: 21.0097\n",
      "Mean absolute Error: 4.8621\n",
      "Mean Squared Error: 441.41\n",
      "Median absolute Error: 0.7300\n",
      "R² Score: 0.5433\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "# Standardize features before autoencoding\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 16 # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "test(knn, X_test_enc, y_test_enc)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Decision tree after autoencoder---->\")\n",
    "test(dt, X_test_enc, y_test_enc)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "test(lr, X_test_enc, y_test_enc)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Random Forest after autoencoder---->\")\n",
    "test(rf, X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de754f5b",
   "metadata": {},
   "source": [
    "Dim 32 Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "479a47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step\n",
      "<----KNN after autoencoder---->\n",
      "Root mean Squared Error: 21.5372\n",
      "Mean absolute Error: 4.7119\n",
      "Mean Squared Error: 463.85\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.5201\n",
      "<----Decision tree after autoencoder---->\n",
      "Root mean Squared Error: 30.7675\n",
      "Mean absolute Error: 6.2853\n",
      "Mean Squared Error: 946.64\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: 0.0207\n",
      "<----Linear Regression after autoencoder---->\n",
      "Root mean Squared Error: 21.0432\n",
      "Mean absolute Error: 5.6267\n",
      "Mean Squared Error: 442.82\n",
      "Median absolute Error: 2.0977\n",
      "R² Score: 0.5419\n",
      "<----Random Forest after autoencoder---->\n",
      "Root mean Squared Error: 21.0101\n",
      "Mean absolute Error: 4.7327\n",
      "Mean Squared Error: 441.43\n",
      "Median absolute Error: 0.6800\n",
      "R² Score: 0.5433\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "# Standardize features before autoencoding\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 32 # Dimension of the encoded representation\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Encode features\n",
    "X_encoded = encoder.predict(X_scaled)\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----KNN after autoencoder---->\")\n",
    "test(knn, X_test_enc, y_test_enc)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Decision tree after autoencoder---->\")\n",
    "test(dt, X_test_enc, y_test_enc)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Linear Regression after autoencoder---->\")\n",
    "test(lr, X_test_enc, y_test_enc)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_enc, y_train_enc)\n",
    "print(\"<----Random Forest after autoencoder---->\")\n",
    "test(rf, X_test_enc, y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2b45a",
   "metadata": {},
   "source": [
    "### Conclusion: What Did We Learn from Using Autoencoders?\n",
    "\n",
    "**Best Configuration:**\n",
    "- Encoder dimension = **32**\n",
    "- Hidden layer size = **64**\n",
    "\n",
    "**Why?**\n",
    "- This setting retained enough information to represent the features well without over-compressing.\n",
    "- It led to the best performance improvement in **K-Nearest Neighbors (KNN)**.\n",
    "\n",
    "**Impact on Models:**\n",
    "- **KNN improved significantly**, because reducing dimensionality helped mitigate the \"curse of dimensionality\" which affects distance-based models.\n",
    "- **Other models (Decision Tree, Linear Regression, Random Forest)** showed little to no improvement, and in some cases performed slightly worse.\n",
    "  - This suggests those models are either more robust to feature space size or already benefit from the full feature information.\n",
    "\n",
    "**Takeaway:**\n",
    "Using a deep autoencoder can be a valuable preprocessing step, especially when:\n",
    "- The input space is high-dimensional\n",
    "- We're using models sensitive to feature space structure (like KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eccafd-f0a9-40ce-99f7-4591a02ac6d5",
   "metadata": {},
   "source": [
    "# PCA\n",
    "When data is linearly separable, simple models like linear regression, logistic regression, or linear SVMs can more easily separate the classes and predict values correctly.\n",
    "To try and improve the linearity of our data, previously we applied autoencoder and decoder to reduce the dimensionality of our dataset. Now, we will apply PCA.\n",
    "\n",
    "#### Why PCA?\n",
    "As seen in class, PCA:\n",
    "\n",
    "- Reduces dimensionality while preserving variance in the data\n",
    "- Eliminates multicollinearity between highly correlated features. This could help because the derived features of our dataset are highly correlated and could assist in reducing redundancy.\n",
    "- Reduces noise that may be present in less relevant features. We think that this point could help with the outliers contained in the data.\n",
    "- Improves computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc490002-9733-4c00-b055-218ec986484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Transform(X_train_, X_test_):    \n",
    "    # 2. Standardize features FIRST\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_)\n",
    "    X_test_scaled = scaler.transform(X_test_)\n",
    "    \n",
    "    # 3. THEN apply PCA to scaled data\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Reduced dimensions from {X_train_.shape[1]} to {X_train_pca.shape[1]} features\")\n",
    "    \n",
    "    return (X_train_pca, X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2636c3a4-8d89-4184-9e67-193b45aeec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions from 53 to 25 features\n",
      "<----KNN---->\n",
      "Root mean Squared Error: 26.5127\n",
      "Mean absolute Error: 5.8084\n",
      "Mean Squared Error: 702.92\n",
      "Median absolute Error: 0.6000\n",
      "R² Score: 0.3078\n",
      "<----Decision Tree---->\n",
      "Root mean Squared Error: 37.8055\n",
      "Mean absolute Error: 7.4654\n",
      "Mean Squared Error: 1429.26\n",
      "Median absolute Error: 1.0000\n",
      "R² Score: -0.4075\n",
      "<----Linear Regression---->\n",
      "Root mean Squared Error: 26.0287\n",
      "Mean absolute Error: 8.6338\n",
      "Mean Squared Error: 677.50\n",
      "Median absolute Error: 4.9093\n",
      "R² Score: 0.3328\n",
      "<----Random Forest---->\n",
      "Root mean Squared Error: 24.0586\n",
      "Mean absolute Error: 5.5502\n",
      "Mean Squared Error: 578.82\n",
      "Median absolute Error: 0.6600\n",
      "R² Score: 0.4300\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=[\"Target Variable\"])\n",
    "y = data[\"Target Variable\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = PCA_Transform(X_train, X_test)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"<----KNN---->\")\n",
    "test(knn, X_test, y_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"<----Decision Tree---->\")\n",
    "test(dt, X_test, y_test)\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"<----Linear Regression---->\")\n",
    "test(lr, X_test, y_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"<----Random Forest---->\")\n",
    "test(rf, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0c19c-178f-4b01-8f4c-23837d55a3c6",
   "metadata": {},
   "source": [
    "### Conclusions: What did we learned from using PCA? \n",
    "Our results demonstrate mixed outcomes across models when applying PCA:\n",
    "\n",
    "- KNN showed clear improvement (RMSE: 27.84 → 26.51, R²: 0.24 → 0.31) as expected for distance-based algorithms. Especially since in class we saw the curse of dimensionality in which KNN is highly sensitive to high dimensional features.\n",
    "- Linear regression remained relatively stable with minimal change. We expected the model to improve slightly because it is a linear model.\n",
    "- Tree-based models (Decision Tree and Random Forest) performed worse after PCA, likely because they already handle feature selection implicitly (and could improve by limiting size of the tree) and we are reducing the amount of information. Also, decision trees are more robust against outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34381ee7-da90-4a2e-93d1-8179484157f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
